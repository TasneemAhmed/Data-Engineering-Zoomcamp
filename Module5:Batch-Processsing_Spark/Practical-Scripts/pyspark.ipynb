{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff586fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d014ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pyspark[connect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78c6986e-2053-455c-be35-e213d770fa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (15.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.16.6 in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pyarrow) (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d206598d-cc56-4005-bd5b-d1c5c8d77895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HADOOP_HOME'] = 'C:/tools/hadoop-3.2.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf5fc983-9c9b-44c5-be54-287cea1544f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c653ac2-411c-402b-84b8-bbb0f1a194af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cause iteritems is deprecated in pandas 2 so replace with items instead of downgrade pandas\n",
    "pd.DataFrame.iteritems = pd.DataFrame.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "835c648a-134d-40bf-b23e-e440656d7814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8edd708-f32d-432d-be87-c7278ae22b51",
   "metadata": {},
   "source": [
    "\n",
    "- SparkSession is an entrypoint for pyspark\n",
    "- master() – If you are running it on the cluster you need to use your master name as an argument to master(). usually, \n",
    "it would be either yarn or mesos depends on your cluster setup.\n",
    "\n",
    "Use local[x] when running in Standalone mode. x should be an integer value and should be greater than 0; this represents how many partitions it should create when using RDD, DataFrame, and Dataset. Ideally, x value should be the number of CPU cores you have.\n",
    "\n",
    "- appName() – Used to set your application name.\n",
    "- \n",
    "getOrCreate() – This returns a SparkSession object if already exists, and creates a new one if not exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e5b8e5-ed27-4d5f-a250-118cfe6483d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#read csv file as spark dataframe\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7efc1fde-1782-4d26-b756-fc0b458090d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11908469 fhvhv_tripdata_2021-01.csv\n"
     ]
    }
   ],
   "source": [
    "#count file records\n",
    "!wc -l \"fhvhv_tripdata_2021-01.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75010d23-5cfc-46b3-95cf-eb5bdb537b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02682|2021-01-01 00:33:44|2021-01-01 00:49:07|         230|         166|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:55:19|2021-01-01 01:18:21|         152|         167|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:23:56|2021-01-01 00:38:05|         233|         142|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:42:51|2021-01-01 00:45:50|         142|         143|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:48:14|2021-01-01 01:08:42|         143|          78|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 00:06:59|2021-01-01 00:43:01|          88|          42|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 00:50:00|2021-01-01 01:04:57|          42|         151|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:14:30|2021-01-01 00:50:27|          71|         226|   NULL|\n",
      "|           HV0003|              B02875|2021-01-01 00:22:54|2021-01-01 00:30:20|         112|         255|   NULL|\n",
      "|           HV0003|              B02875|2021-01-01 00:40:12|2021-01-01 00:53:31|         255|         232|   NULL|\n",
      "|           HV0003|              B02875|2021-01-01 00:56:45|2021-01-01 01:17:42|         232|         198|   NULL|\n",
      "|           HV0003|              B02835|2021-01-01 00:29:04|2021-01-01 00:36:27|         113|          48|   NULL|\n",
      "|           HV0003|              B02835|2021-01-01 00:48:56|2021-01-01 00:59:12|         239|          75|   NULL|\n",
      "|           HV0004|              B02800|2021-01-01 00:15:24|2021-01-01 00:38:31|         181|         237|   NULL|\n",
      "|           HV0004|              B02800|2021-01-01 00:45:00|2021-01-01 01:06:45|         236|          68|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:11:53|2021-01-01 00:18:06|         256|         148|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:28:31|2021-01-01 00:41:40|          79|          80|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:50:49|2021-01-01 00:55:59|          17|         217|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 00:08:40|2021-01-01 00:39:39|          62|          29|   NULL|\n",
      "|           HV0003|              B02836|2021-01-01 00:53:48|2021-01-01 01:11:40|          22|          22|   NULL|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ccf8cd-ca42-4c32-b6f8-298dd1ae30f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime='2021-01-01 00:33:44', dropoff_datetime='2021-01-01 00:49:07', PULocationID='230', DOLocationID='166', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02682', pickup_datetime='2021-01-01 00:55:19', dropoff_datetime='2021-01-01 01:18:21', PULocationID='152', DOLocationID='167', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:23:56', dropoff_datetime='2021-01-01 00:38:05', PULocationID='233', DOLocationID='142', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:42:51', dropoff_datetime='2021-01-01 00:45:50', PULocationID='142', DOLocationID='143', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:48:14', dropoff_datetime='2021-01-01 01:08:42', PULocationID='143', DOLocationID='78', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0005', dispatching_base_num='B02510', pickup_datetime='2021-01-01 00:06:59', dropoff_datetime='2021-01-01 00:43:01', PULocationID='88', DOLocationID='42', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0005', dispatching_base_num='B02510', pickup_datetime='2021-01-01 00:50:00', dropoff_datetime='2021-01-01 01:04:57', PULocationID='42', DOLocationID='151', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', pickup_datetime='2021-01-01 00:14:30', dropoff_datetime='2021-01-01 00:50:27', PULocationID='71', DOLocationID='226', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02875', pickup_datetime='2021-01-01 00:22:54', dropoff_datetime='2021-01-01 00:30:20', PULocationID='112', DOLocationID='255', SR_Flag=None),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02875', pickup_datetime='2021-01-01 00:40:12', dropoff_datetime='2021-01-01 00:53:31', PULocationID='255', DOLocationID='232', SR_Flag=None)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b32aedf6-bc93-4e5e-b1ce-7755c72726f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1001 \"fhvhv_tripdata_2021-01.csv\" > head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58b9edde-1ef5-4646-8b2a-6912847bb541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001 head.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c999ab7-8676-490f-8bb7-97afea1615b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dba8e605-706c-4df6-b140-d3b95e714552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hvfhs_license_num</th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02682</td>\n",
       "      <td>2021-01-01 00:33:44</td>\n",
       "      <td>2021-01-01 00:49:07</td>\n",
       "      <td>230</td>\n",
       "      <td>166</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02682</td>\n",
       "      <td>2021-01-01 00:55:19</td>\n",
       "      <td>2021-01-01 01:18:21</td>\n",
       "      <td>152</td>\n",
       "      <td>167</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-01-01 00:23:56</td>\n",
       "      <td>2021-01-01 00:38:05</td>\n",
       "      <td>233</td>\n",
       "      <td>142</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-01-01 00:42:51</td>\n",
       "      <td>2021-01-01 00:45:50</td>\n",
       "      <td>142</td>\n",
       "      <td>143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02764</td>\n",
       "      <td>2021-01-01 00:48:14</td>\n",
       "      <td>2021-01-01 01:08:42</td>\n",
       "      <td>143</td>\n",
       "      <td>78</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hvfhs_license_num dispatching_base_num      pickup_datetime  \\\n",
       "0            HV0003               B02682  2021-01-01 00:33:44   \n",
       "1            HV0003               B02682  2021-01-01 00:55:19   \n",
       "2            HV0003               B02764  2021-01-01 00:23:56   \n",
       "3            HV0003               B02764  2021-01-01 00:42:51   \n",
       "4            HV0003               B02764  2021-01-01 00:48:14   \n",
       "\n",
       "      dropoff_datetime  PULocationID  DOLocationID  SR_Flag  \n",
       "0  2021-01-01 00:49:07           230           166      NaN  \n",
       "1  2021-01-01 01:18:21           152           167      NaN  \n",
       "2  2021-01-01 00:38:05           233           142      NaN  \n",
       "3  2021-01-01 00:45:50           142           143      NaN  \n",
       "4  2021-01-01 01:08:42           143            78      NaN  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas = pd.read_csv(\"head.csv\")\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d96372b-2fc6-49c1-8476-cb5eef7475c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num        object\n",
       "dispatching_base_num     object\n",
       "pickup_datetime          object\n",
       "dropoff_datetime         object\n",
       "PULocationID              int64\n",
       "DOLocationID              int64\n",
       "SR_Flag                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26818a4-991f-4b5b-9ba0-d33f5f775d9b",
   "metadata": {},
   "source": [
    "Spark hasn't infer schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff8a717c-002e-4dc6-a3fa-8c8a60520349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "#Create User defined Custom Schema using StructType, True means Nullable\n",
    "mySchema = types.StructType([ types.StructField(\"hvfhs_license_num\", types.StringType(), True)\\\n",
    "                       ,types.StructField(\"dispatching_base_num\", types.StringType(), True)\\\n",
    "                       ,types.StructField(\"pickup_datetime\", types.TimestampType(), True)\\\n",
    "                       ,types.StructField(\"dropoff_datetime\", types.TimestampType(), True)\\\n",
    "                       ,types.StructField(\"PULocationID\", types.IntegerType(), True)\\\n",
    "                       ,types.StructField(\"DOLocationID\", types.IntegerType(), True)\\\n",
    "                       ,types.StructField(\"SR_Flag\", types.FloatType(), True)\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a493ebf-9314-48a9-a0dd-0cbd821d1d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02682|2021-01-01 00:33:44|2021-01-01 00:49:07|         230|         166|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:55:19|2021-01-01 01:18:21|         152|         167|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:23:56|2021-01-01 00:38:05|         233|         142|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:42:51|2021-01-01 00:45:50|         142|         143|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:48:14|2021-01-01 01:08:42|         143|          78|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 00:06:59|2021-01-01 00:43:01|          88|          42|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 00:50:00|2021-01-01 01:04:57|          42|         151|   NULL|\n",
      "|           HV0003|              B02764|2021-01-01 00:14:30|2021-01-01 00:50:27|          71|         226|   NULL|\n",
      "|           HV0003|              B02875|2021-01-01 00:22:54|2021-01-01 00:30:20|         112|         255|   NULL|\n",
      "|           HV0003|              B02875|2021-01-01 00:40:12|2021-01-01 00:53:31|         255|         232|   NULL|\n",
      "|           HV0003|              B02875|2021-01-01 00:56:45|2021-01-01 01:17:42|         232|         198|   NULL|\n",
      "|           HV0003|              B02835|2021-01-01 00:29:04|2021-01-01 00:36:27|         113|          48|   NULL|\n",
      "|           HV0003|              B02835|2021-01-01 00:48:56|2021-01-01 00:59:12|         239|          75|   NULL|\n",
      "|           HV0004|              B02800|2021-01-01 00:15:24|2021-01-01 00:38:31|         181|         237|   NULL|\n",
      "|           HV0004|              B02800|2021-01-01 00:45:00|2021-01-01 01:06:45|         236|          68|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:11:53|2021-01-01 00:18:06|         256|         148|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:28:31|2021-01-01 00:41:40|          79|          80|   NULL|\n",
      "|           HV0003|              B02682|2021-01-01 00:50:49|2021-01-01 00:55:59|          17|         217|   NULL|\n",
      "|           HV0005|              B02510|2021-01-01 00:08:40|2021-01-01 00:39:39|          62|          29|   NULL|\n",
      "|           HV0003|              B02836|2021-01-01 00:53:48|2021-01-01 01:11:40|          22|          22|   NULL|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read file as spark dataframe\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(mySchema)\\\n",
    "    .csv('fhvhv_tripdata_2021-01.csv')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "132b178b-d7fd-46c9-8ef0-cd6ec52b45d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hvfhs_license_num', 'string'),\n",
       " ('dispatching_base_num', 'string'),\n",
       " ('pickup_datetime', 'timestamp'),\n",
       " ('dropoff_datetime', 'timestamp'),\n",
       " ('PULocationID', 'int'),\n",
       " ('DOLocationID', 'int'),\n",
       " ('SR_Flag', 'float')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98888e51-6616-4961-b03c-20766c476595",
   "metadata": {},
   "source": [
    "- select & filter are transformation(lazy) because of not excuted untill apply action like: show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c94142bb-1307-40b7-be78-b8f7937c5102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|\n",
      "+--------------------+-------------------+-------------------+\n",
      "|              B02682|2021-01-01 00:33:44|2021-01-01 00:49:07|\n",
      "|              B02682|2021-01-01 00:55:19|2021-01-01 01:18:21|\n",
      "|              B02764|2021-01-01 00:23:56|2021-01-01 00:38:05|\n",
      "|              B02764|2021-01-01 00:42:51|2021-01-01 00:45:50|\n",
      "|              B02764|2021-01-01 00:48:14|2021-01-01 01:08:42|\n",
      "|              B02764|2021-01-01 00:14:30|2021-01-01 00:50:27|\n",
      "|              B02875|2021-01-01 00:22:54|2021-01-01 00:30:20|\n",
      "|              B02875|2021-01-01 00:40:12|2021-01-01 00:53:31|\n",
      "|              B02875|2021-01-01 00:56:45|2021-01-01 01:17:42|\n",
      "|              B02835|2021-01-01 00:29:04|2021-01-01 00:36:27|\n",
      "|              B02835|2021-01-01 00:48:56|2021-01-01 00:59:12|\n",
      "|              B02682|2021-01-01 00:11:53|2021-01-01 00:18:06|\n",
      "|              B02682|2021-01-01 00:28:31|2021-01-01 00:41:40|\n",
      "|              B02682|2021-01-01 00:50:49|2021-01-01 00:55:59|\n",
      "|              B02836|2021-01-01 00:53:48|2021-01-01 01:11:40|\n",
      "|              B02836|2021-01-01 00:36:21|2021-01-01 00:57:41|\n",
      "|              B02512|2021-01-01 00:14:21|2021-01-01 00:21:16|\n",
      "|              B02512|2021-01-01 00:26:37|2021-01-01 01:08:20|\n",
      "|              B02764|2021-01-01 00:19:11|2021-01-01 00:33:49|\n",
      "|              B02764|2021-01-01 00:39:51|2021-01-01 00:58:51|\n",
      "+--------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('dispatching_base_num', 'pickup_datetime', 'dropoff_datetime')\\\n",
    ".filter(df.hvfhs_license_num == 'HV0003').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "943384fd-ea34-42eb-9c0b-4e26b86c81b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import built in functions from pyspark\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05da3aa2-de38-43ed-9a45-7e4c164481fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+-------------------+------------+\n",
      "|    pickup_datetime|pickup_date|   dropoff_datetime|dropoff_date|\n",
      "+-------------------+-----------+-------------------+------------+\n",
      "|2021-01-01 00:33:44| 2021-01-01|2021-01-01 00:49:07|  2021-01-01|\n",
      "|2021-01-01 00:55:19| 2021-01-01|2021-01-01 01:18:21|  2021-01-01|\n",
      "|2021-01-01 00:23:56| 2021-01-01|2021-01-01 00:38:05|  2021-01-01|\n",
      "|2021-01-01 00:42:51| 2021-01-01|2021-01-01 00:45:50|  2021-01-01|\n",
      "|2021-01-01 00:48:14| 2021-01-01|2021-01-01 01:08:42|  2021-01-01|\n",
      "|2021-01-01 00:06:59| 2021-01-01|2021-01-01 00:43:01|  2021-01-01|\n",
      "|2021-01-01 00:50:00| 2021-01-01|2021-01-01 01:04:57|  2021-01-01|\n",
      "|2021-01-01 00:14:30| 2021-01-01|2021-01-01 00:50:27|  2021-01-01|\n",
      "|2021-01-01 00:22:54| 2021-01-01|2021-01-01 00:30:20|  2021-01-01|\n",
      "|2021-01-01 00:40:12| 2021-01-01|2021-01-01 00:53:31|  2021-01-01|\n",
      "|2021-01-01 00:56:45| 2021-01-01|2021-01-01 01:17:42|  2021-01-01|\n",
      "|2021-01-01 00:29:04| 2021-01-01|2021-01-01 00:36:27|  2021-01-01|\n",
      "|2021-01-01 00:48:56| 2021-01-01|2021-01-01 00:59:12|  2021-01-01|\n",
      "|2021-01-01 00:15:24| 2021-01-01|2021-01-01 00:38:31|  2021-01-01|\n",
      "|2021-01-01 00:45:00| 2021-01-01|2021-01-01 01:06:45|  2021-01-01|\n",
      "|2021-01-01 00:11:53| 2021-01-01|2021-01-01 00:18:06|  2021-01-01|\n",
      "|2021-01-01 00:28:31| 2021-01-01|2021-01-01 00:41:40|  2021-01-01|\n",
      "|2021-01-01 00:50:49| 2021-01-01|2021-01-01 00:55:59|  2021-01-01|\n",
      "|2021-01-01 00:08:40| 2021-01-01|2021-01-01 00:39:39|  2021-01-01|\n",
      "|2021-01-01 00:53:48| 2021-01-01|2021-01-01 01:11:40|  2021-01-01|\n",
      "+-------------------+-----------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.withColumn(): it is spark dataframe function used to add a new or update an existing column on DataFrame\n",
    "#f.to_date pyspark sql function which convert column to date\n",
    "df\\\n",
    ".withColumn('pickup_date', f.to_date(df.pickup_datetime))\\\n",
    "            .withColumn('dropoff_date', f.to_date(df.dropoff_datetime))\\\n",
    "            .select('pickup_datetime', 'pickup_date', 'dropoff_datetime', 'dropoff_date')\\\n",
    "            .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18fdd855-9b09-4801-b547-81f9a9e8a195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_char(base_number):\n",
    "   return base_number[0].lower() + base_number[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c668c3ac-d266-416a-83a1-111303c3cce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b0276'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_char('B0276')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2bbc237-20ae-4209-aa83-63abfe10465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "#to register upper function as user defined function to use in dataframe or SQL\n",
    "lower_func = udf(lambda x : lower_char(x), returnType = types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9693fe92-3fee-4270-8963-76a05fd9df9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+----------------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|dispatching_base|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+----------------+\n",
      "|           HV0003|              B02682|2021-01-01 00:33:44|2021-01-01 00:49:07|         230|         166|   NULL|          b02682|\n",
      "|           HV0003|              B02682|2021-01-01 00:55:19|2021-01-01 01:18:21|         152|         167|   NULL|          b02682|\n",
      "|           HV0003|              B02764|2021-01-01 00:23:56|2021-01-01 00:38:05|         233|         142|   NULL|          b02764|\n",
      "|           HV0003|              B02764|2021-01-01 00:42:51|2021-01-01 00:45:50|         142|         143|   NULL|          b02764|\n",
      "|           HV0003|              B02764|2021-01-01 00:48:14|2021-01-01 01:08:42|         143|          78|   NULL|          b02764|\n",
      "|           HV0005|              B02510|2021-01-01 00:06:59|2021-01-01 00:43:01|          88|          42|   NULL|          b02510|\n",
      "|           HV0005|              B02510|2021-01-01 00:50:00|2021-01-01 01:04:57|          42|         151|   NULL|          b02510|\n",
      "|           HV0003|              B02764|2021-01-01 00:14:30|2021-01-01 00:50:27|          71|         226|   NULL|          b02764|\n",
      "|           HV0003|              B02875|2021-01-01 00:22:54|2021-01-01 00:30:20|         112|         255|   NULL|          b02875|\n",
      "|           HV0003|              B02875|2021-01-01 00:40:12|2021-01-01 00:53:31|         255|         232|   NULL|          b02875|\n",
      "|           HV0003|              B02875|2021-01-01 00:56:45|2021-01-01 01:17:42|         232|         198|   NULL|          b02875|\n",
      "|           HV0003|              B02835|2021-01-01 00:29:04|2021-01-01 00:36:27|         113|          48|   NULL|          b02835|\n",
      "|           HV0003|              B02835|2021-01-01 00:48:56|2021-01-01 00:59:12|         239|          75|   NULL|          b02835|\n",
      "|           HV0004|              B02800|2021-01-01 00:15:24|2021-01-01 00:38:31|         181|         237|   NULL|          b02800|\n",
      "|           HV0004|              B02800|2021-01-01 00:45:00|2021-01-01 01:06:45|         236|          68|   NULL|          b02800|\n",
      "|           HV0003|              B02682|2021-01-01 00:11:53|2021-01-01 00:18:06|         256|         148|   NULL|          b02682|\n",
      "|           HV0003|              B02682|2021-01-01 00:28:31|2021-01-01 00:41:40|          79|          80|   NULL|          b02682|\n",
      "|           HV0003|              B02682|2021-01-01 00:50:49|2021-01-01 00:55:59|          17|         217|   NULL|          b02682|\n",
      "|           HV0005|              B02510|2021-01-01 00:08:40|2021-01-01 00:39:39|          62|          29|   NULL|          b02510|\n",
      "|           HV0003|              B02836|2021-01-01 00:53:48|2021-01-01 01:11:40|          22|          22|   NULL|          b02836|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\\\n",
    ".withColumn('dispatching_base', lower_func(col(\"dispatching_base_num\")))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dfb1269-4dc1-40ba-a266-b7c8d0675174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#partition dataframe into 24 partions\n",
    "# it's a lazy command, which is run when call it\n",
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a32d3d-fca2-44be-8ef5-5904c1eebca7",
   "metadata": {},
   "source": [
    "when run below command give me error: Py4JJavaError\n",
    "Python 3.11 is not yet stable for PySpark. So, make sure you're setting up your virtualenv with either Python 3.9 or Python 3.10\n",
    "Solution: \n",
    "$ dd$ pip install --no-cashe-dir pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47ed8de5-2e7e-4e79-8802-b36f6184488d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.write.parquet('fhvhv/2021/01_temp/', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ccf927-6820-4099-8432-75d6e7af1d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
